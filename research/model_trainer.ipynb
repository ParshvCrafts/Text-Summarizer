{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07bfd8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c04ddf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int  \n",
    "    weight_decay: float\n",
    "    learning_rate: float  \n",
    "    logging_steps: int\n",
    "    eval_strategy: str\n",
    "    eval_steps: int\n",
    "    save_strategy: str \n",
    "    save_steps: int\n",
    "    save_total_limit: int  \n",
    "    gradient_accumulation_steps: int\n",
    "    lr_scheduler_type: str  \n",
    "    fp16: bool  \n",
    "    load_best_model_at_end: bool  \n",
    "    metric_for_best_model: str  \n",
    "    greater_is_better: bool  \n",
    "    prediction_loss_only: bool  \n",
    "    remove_unused_columns: bool  \n",
    "    report_to: str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ef9bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_summarizer.logger import logger\n",
    "from src.text_summarizer.utils.common import read_yaml, create_directories\n",
    "from src.text_summarizer.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81dec446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        # Check if config files exist\n",
    "        if not os.path.exists(config_filepath):\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {config_filepath}\")\n",
    "        if not os.path.exists(params_filepath):\n",
    "            raise FileNotFoundError(f\"Parameters file not found: {params_filepath}\")\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Check if artifacts_root exists in config\n",
    "        if not hasattr(self.config, 'artifacts_root'):\n",
    "            raise KeyError(\"'artifacts_root' key not found in config.yaml. Please add: artifacts_root: artifacts\")\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "        \n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        # Convert string values to appropriate types\n",
    "        model_trainer_config = ModelTrainingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(config.data_path),\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            num_train_epochs=int(params.num_train_epochs),\n",
    "            warmup_steps=int(params.warmup_steps),\n",
    "            per_device_train_batch_size=int(params.per_device_train_batch_size),\n",
    "            per_device_eval_batch_size=int(params.per_device_eval_batch_size),  \n",
    "            weight_decay=float(params.weight_decay),\n",
    "            learning_rate=float(params.learning_rate),  \n",
    "            logging_steps=int(params.logging_steps),\n",
    "            eval_strategy=params.eval_strategy,\n",
    "            eval_steps=int(params.eval_steps),\n",
    "            save_strategy=params.save_strategy,  \n",
    "            save_steps=int(params.save_steps),\n",
    "            save_total_limit=int(params.save_total_limit),  \n",
    "            gradient_accumulation_steps=int(params.gradient_accumulation_steps),\n",
    "            lr_scheduler_type=params.lr_scheduler_type,  \n",
    "            fp16=bool(params.fp16),  \n",
    "            load_best_model_at_end=bool(params.load_best_model_at_end),  \n",
    "            metric_for_best_model=params.metric_for_best_model,  \n",
    "            greater_is_better=bool(params.greater_is_better),  \n",
    "            prediction_loss_only=bool(params.prediction_loss_only),  \n",
    "            remove_unused_columns=bool(params.remove_unused_columns),  \n",
    "            report_to=params.report_to  \n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2e573bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cf80ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def train(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "        \n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "        \n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,  # Use specific eval batch size\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            learning_rate=self.config.learning_rate,  # Added\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            eval_strategy=self.config.eval_strategy,  # Note: evaluation_strategy not eval_strategy\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            save_strategy=self.config.save_strategy,  # Added\n",
    "            save_steps=self.config.save_steps,\n",
    "            save_total_limit=self.config.save_total_limit,  # Added\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            lr_scheduler_type=self.config.lr_scheduler_type,  # Added\n",
    "            fp16=self.config.fp16,  # Added - mixed precision training\n",
    "            load_best_model_at_end=self.config.load_best_model_at_end,  # Added\n",
    "            metric_for_best_model=self.config.metric_for_best_model,  # Added\n",
    "            greater_is_better=self.config.greater_is_better,  # Added\n",
    "            prediction_loss_only=self.config.prediction_loss_only,  # Added\n",
    "            remove_unused_columns=self.config.remove_unused_columns,  # Added\n",
    "            report_to=self.config.report_to,  # Added\n",
    "            dataloader_pin_memory=True,  # Added for performance\n",
    "            dataloader_num_workers=2,  # Added for performance\n",
    "            logging_dir=os.path.join(self.config.root_dir, \"logs\"),  # Added for logging\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model_pegasus,\n",
    "            args=trainer_args,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=seq2seq_data_collator,\n",
    "            train_dataset=dataset_samsum_pt[\"train\"],\n",
    "            eval_dataset=dataset_samsum_pt[\"validation\"]\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the best model (automatically loaded if load_best_model_at_end=True)\n",
    "        trainer.save_model()\n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir, \"pegasus_model\"))\n",
    "        \n",
    "        # Also save tokenizer\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir, \"tokenizer\"))\n",
    "        \n",
    "        logger.info(\"Model training completed successfully!\")\n",
    "        logger.info(f\"Best model saved at: {self.config.root_dir}\")\n",
    "        logger.info(f\"Final evaluation loss: {trainer.state.log_history[-1].get('eval_loss', 'N/A')}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4aff917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-10 20:33:32,975: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2025-09-10 20:33:32,975: INFO: common]: yaml file: config\\params.yaml loaded successfully\n",
      "[2025-09-10 20:33:32,981: INFO: common]: Directory created at: artifacts\n",
      "[2025-09-10 20:33:32,982: INFO: common]: Directory created at: artifacts/model_trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-10 20:33:40,926: ERROR: 4202113604]: Error training model: Invalid version: 'N/A'\n"
     ]
    },
    {
     "ename": "InvalidVersion",
     "evalue": "Invalid version: 'N/A'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidVersion\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      8\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError training model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m     model_trainer_config = config_manager.get_model_training_config()\n\u001b[32m      4\u001b[39m     model_trainer = ModelTrainer(model_trainer_config)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mmodel_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      8\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError training model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      9\u001b[39m seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n\u001b[32m     11\u001b[39m dataset_samsum_pt = load_from_disk(\u001b[38;5;28mself\u001b[39m.config.data_path)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m trainer_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use specific eval batch size\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Note: evaluation_strategy not eval_strategy\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added - mixed precision training\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_pin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added for performance\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added for performance\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added for logging\u001b[39;49;00m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m trainer = Trainer(\n\u001b[32m     42\u001b[39m     model=model_pegasus,\n\u001b[32m     43\u001b[39m     args=trainer_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m     eval_dataset=dataset_samsum_pt[\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     48\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:134\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\training_args.py:1780\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1777\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim = OptimizerNames.ADAFACTOR\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# We need to setup the accelerator config here *before* the first call to `self.device`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_accelerate_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.accelerator_config, AcceleratorConfig):\n\u001b[32m   1782\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:399\u001b[39m, in \u001b[36mis_accelerate_available\u001b[39m\u001b[34m(min_version)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_accelerate_available\u001b[39m(min_version: \u001b[38;5;28mstr\u001b[39m = ACCELERATE_MIN_VERSION) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _accelerate_available \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mversion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accelerate_version\u001b[49m\u001b[43m)\u001b[49m >= version.parse(min_version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\packaging\\version.py:56\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(version)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(version: \u001b[38;5;28mstr\u001b[39m) -> Version:\n\u001b[32m     48\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the given version string.\u001b[39;00m\n\u001b[32m     49\u001b[39m \n\u001b[32m     50\u001b[39m \u001b[33;03m    >>> parse('1.0.dev1')\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m \u001b[33;03m    :raises InvalidVersion: When the version string is not a valid version.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\packaging\\version.py:202\u001b[39m, in \u001b[36mVersion.__init__\u001b[39m\u001b[34m(self, version)\u001b[39m\n\u001b[32m    200\u001b[39m match = \u001b[38;5;28mself\u001b[39m._regex.search(version)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidVersion(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# Store the parsed out pieces of the version\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;28mself\u001b[39m._version = _Version(\n\u001b[32m    206\u001b[39m     epoch=\u001b[38;5;28mint\u001b[39m(match.group(\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m match.group(\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m    207\u001b[39m     release=\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m match.group(\u001b[33m\"\u001b[39m\u001b[33mrelease\u001b[39m\u001b[33m\"\u001b[39m).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m    213\u001b[39m     local=_parse_local_version(match.group(\u001b[33m\"\u001b[39m\u001b[33mlocal\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m    214\u001b[39m )\n",
      "\u001b[31mInvalidVersion\u001b[39m: Invalid version: 'N/A'"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    config_manager = ConfigurationManager()\n",
    "    model_trainer_config = config_manager.get_model_training_config()\n",
    "    model_trainer = ModelTrainer(model_trainer_config)\n",
    "    model_trainer.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error training model: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b11316f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-10 20:33:49,749: INFO: 2779877579]: Accelerate version: 1.10.1\n",
      "[2025-09-10 20:33:49,752: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2025-09-10 20:33:49,755: INFO: common]: yaml file: config\\params.yaml loaded successfully\n",
      "[2025-09-10 20:33:49,756: INFO: common]: Directory created at: artifacts\n",
      "[2025-09-10 20:33:49,757: INFO: common]: Directory created at: artifacts/model_trainer\n",
      "[2025-09-10 20:33:49,758: INFO: 2779877579]: Using device: cpu\n",
      "[2025-09-10 20:33:49,759: INFO: 2779877579]: Loading model from: google/pegasus-cnn_dailymail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-10 20:33:56,478: INFO: 2779877579]: Loading dataset from: artifacts\\data_transformation\\samsum_dataset\n",
      "[2025-09-10 20:33:56,496: ERROR: 2779877579]: Error during training: Invalid version: 'N/A'\n",
      "[2025-09-10 20:33:56,497: ERROR: 2779877579]: Error training model: Invalid version: 'N/A'\n"
     ]
    },
    {
     "ename": "InvalidVersion",
     "evalue": "Invalid version: 'N/A'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidVersion\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 220\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    219\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError training model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 216\u001b[39m\n\u001b[32m    214\u001b[39m     model_trainer_config = config_manager.get_model_training_config()\n\u001b[32m    215\u001b[39m     model_trainer = ModelTrainer(model_trainer_config)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[43mmodel_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    219\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError training model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m os.makedirs(logs_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Training arguments\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m trainer_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_pin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False to avoid potential issues\u001b[39;49;00m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Set to 0 to avoid multiprocessing issues\u001b[39;49;00m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogs_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Explicitly set ddp_find_unused_parameters to False to avoid warnings\u001b[39;49;00m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mddp_find_unused_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Initialize trainer\u001b[39;00m\n\u001b[32m    170\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mInitializing Trainer...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:134\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\training_args.py:1780\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1777\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim = OptimizerNames.ADAFACTOR\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# We need to setup the accelerator config here *before* the first call to `self.device`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_accelerate_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.accelerator_config, AcceleratorConfig):\n\u001b[32m   1782\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:399\u001b[39m, in \u001b[36mis_accelerate_available\u001b[39m\u001b[34m(min_version)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_accelerate_available\u001b[39m(min_version: \u001b[38;5;28mstr\u001b[39m = ACCELERATE_MIN_VERSION) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _accelerate_available \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mversion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accelerate_version\u001b[49m\u001b[43m)\u001b[49m >= version.parse(min_version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\packaging\\version.py:56\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(version)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(version: \u001b[38;5;28mstr\u001b[39m) -> Version:\n\u001b[32m     48\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the given version string.\u001b[39;00m\n\u001b[32m     49\u001b[39m \n\u001b[32m     50\u001b[39m \u001b[33;03m    >>> parse('1.0.dev1')\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m \u001b[33;03m    :raises InvalidVersion: When the version string is not a valid version.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\packaging\\version.py:202\u001b[39m, in \u001b[36mVersion.__init__\u001b[39m\u001b[34m(self, version)\u001b[39m\n\u001b[32m    200\u001b[39m match = \u001b[38;5;28mself\u001b[39m._regex.search(version)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidVersion(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# Store the parsed out pieces of the version\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;28mself\u001b[39m._version = _Version(\n\u001b[32m    206\u001b[39m     epoch=\u001b[38;5;28mint\u001b[39m(match.group(\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m match.group(\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m    207\u001b[39m     release=\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m match.group(\u001b[33m\"\u001b[39m\u001b[33mrelease\u001b[39m\u001b[33m\"\u001b[39m).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m    213\u001b[39m     local=_parse_local_version(match.group(\u001b[33m\"\u001b[39m\u001b[33mlocal\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m    214\u001b[39m )\n",
      "\u001b[31mInvalidVersion\u001b[39m: Invalid version: 'N/A'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int  \n",
    "    weight_decay: float\n",
    "    learning_rate: float  \n",
    "    logging_steps: int\n",
    "    eval_strategy: str\n",
    "    eval_steps: int\n",
    "    save_strategy: str \n",
    "    save_steps: int\n",
    "    save_total_limit: int  \n",
    "    gradient_accumulation_steps: int\n",
    "    lr_scheduler_type: str  \n",
    "    fp16: bool  \n",
    "    load_best_model_at_end: bool  \n",
    "    metric_for_best_model: str  \n",
    "    greater_is_better: bool  \n",
    "    prediction_loss_only: bool  \n",
    "    remove_unused_columns: bool  \n",
    "    report_to: str  \n",
    "\n",
    "from src.text_summarizer.logger import logger\n",
    "from src.text_summarizer.utils.common import read_yaml, create_directories\n",
    "from src.text_summarizer.constants import *\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        # Check if config files exist\n",
    "        if not os.path.exists(config_filepath):\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {config_filepath}\")\n",
    "        if not os.path.exists(params_filepath):\n",
    "            raise FileNotFoundError(f\"Parameters file not found: {params_filepath}\")\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Check if artifacts_root exists in config\n",
    "        if not hasattr(self.config, 'artifacts_root'):\n",
    "            raise KeyError(\"'artifacts_root' key not found in config.yaml. Please add: artifacts_root: artifacts\")\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "        \n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        # Convert string values to appropriate types\n",
    "        model_trainer_config = ModelTrainingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(config.data_path),\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            num_train_epochs=int(params.num_train_epochs),\n",
    "            warmup_steps=int(params.warmup_steps),\n",
    "            per_device_train_batch_size=int(params.per_device_train_batch_size),\n",
    "            per_device_eval_batch_size=int(params.per_device_eval_batch_size),  \n",
    "            weight_decay=float(params.weight_decay),\n",
    "            learning_rate=float(params.learning_rate),  \n",
    "            logging_steps=int(params.logging_steps),\n",
    "            eval_strategy=params.eval_strategy,\n",
    "            eval_steps=int(params.eval_steps),\n",
    "            save_strategy=params.save_strategy,  \n",
    "            save_steps=int(params.save_steps),\n",
    "            save_total_limit=int(params.save_total_limit),  \n",
    "            gradient_accumulation_steps=int(params.gradient_accumulation_steps),\n",
    "            lr_scheduler_type=params.lr_scheduler_type,  \n",
    "            fp16=bool(params.fp16),  \n",
    "            load_best_model_at_end=bool(params.load_best_model_at_end),  \n",
    "            metric_for_best_model=params.metric_for_best_model,  \n",
    "            greater_is_better=bool(params.greater_is_better),  \n",
    "            prediction_loss_only=bool(params.prediction_loss_only),  \n",
    "            remove_unused_columns=bool(params.remove_unused_columns),  \n",
    "            report_to=params.report_to  \n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "\n",
    "# Import all required packages at the top\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# Ensure accelerate is available before using Trainer\n",
    "try:\n",
    "    import accelerate\n",
    "    logger.info(f\"Accelerate version: {accelerate.__version__}\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\"Accelerate not available: {e}\")\n",
    "    raise ImportError(\"Accelerate is required for training. Please install with: pip install accelerate>=0.26.0\")\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def train(self):\n",
    "        try:\n",
    "            # Set up device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            logger.info(f\"Using device: {device}\")\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            logger.info(f\"Loading model from: {self.config.model_ckpt}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "            model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "            \n",
    "            # Data collator\n",
    "            seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "            \n",
    "            # Load dataset\n",
    "            logger.info(f\"Loading dataset from: {self.config.data_path}\")\n",
    "            dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "            \n",
    "            # Create logs directory\n",
    "            logs_dir = os.path.join(self.config.root_dir, \"logs\")\n",
    "            os.makedirs(logs_dir, exist_ok=True)\n",
    "            \n",
    "            # Training arguments\n",
    "            trainer_args = TrainingArguments(\n",
    "                output_dir=self.config.root_dir,\n",
    "                num_train_epochs=self.config.num_train_epochs,\n",
    "                warmup_steps=self.config.warmup_steps,\n",
    "                per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "                per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "                learning_rate=self.config.learning_rate,\n",
    "                logging_steps=self.config.logging_steps,\n",
    "                eval_strategy=self.config.eval_strategy,\n",
    "                eval_steps=self.config.eval_steps,\n",
    "                save_strategy=self.config.save_strategy,\n",
    "                save_steps=self.config.save_steps,\n",
    "                save_total_limit=self.config.save_total_limit,\n",
    "                gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "                lr_scheduler_type=self.config.lr_scheduler_type,\n",
    "                fp16=self.config.fp16,\n",
    "                load_best_model_at_end=self.config.load_best_model_at_end,\n",
    "                metric_for_best_model=self.config.metric_for_best_model,\n",
    "                greater_is_better=self.config.greater_is_better,\n",
    "                prediction_loss_only=self.config.prediction_loss_only,\n",
    "                remove_unused_columns=self.config.remove_unused_columns,\n",
    "                report_to=self.config.report_to,\n",
    "                dataloader_pin_memory=False,  # Set to False to avoid potential issues\n",
    "                dataloader_num_workers=0,     # Set to 0 to avoid multiprocessing issues\n",
    "                logging_dir=logs_dir,\n",
    "                # Explicitly set ddp_find_unused_parameters to False to avoid warnings\n",
    "                ddp_find_unused_parameters=False,\n",
    "            )\n",
    "            \n",
    "            # Initialize trainer\n",
    "            logger.info(\"Initializing Trainer...\")\n",
    "            trainer = Trainer(\n",
    "                model=model_pegasus,\n",
    "                args=trainer_args,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=seq2seq_data_collator,\n",
    "                train_dataset=dataset_samsum_pt[\"train\"],\n",
    "                eval_dataset=dataset_samsum_pt[\"validation\"]\n",
    "            )\n",
    "            \n",
    "            # Start training\n",
    "            logger.info(\"Starting training...\")\n",
    "            trainer.train()\n",
    "            \n",
    "            # Save the model\n",
    "            logger.info(\"Saving model...\")\n",
    "            trainer.save_model()\n",
    "            \n",
    "            # Save model and tokenizer separately\n",
    "            model_dir = os.path.join(self.config.root_dir, \"pegasus_model\")\n",
    "            tokenizer_dir = os.path.join(self.config.root_dir, \"tokenizer\")\n",
    "            \n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "            \n",
    "            model_pegasus.save_pretrained(model_dir)\n",
    "            tokenizer.save_pretrained(tokenizer_dir)\n",
    "            \n",
    "            logger.info(\"Model training completed successfully!\")\n",
    "            logger.info(f\"Model saved at: {model_dir}\")\n",
    "            logger.info(f\"Tokenizer saved at: {tokenizer_dir}\")\n",
    "            \n",
    "            # Log final metrics\n",
    "            if trainer.state.log_history:\n",
    "                final_metrics = trainer.state.log_history[-1]\n",
    "                logger.info(f\"Final training metrics: {final_metrics}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Main execution\n",
    "try: \n",
    "    config_manager = ConfigurationManager()\n",
    "    model_trainer_config = config_manager.get_model_training_config()\n",
    "    model_trainer = ModelTrainer(model_trainer_config)\n",
    "    model_trainer.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error training model: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1252f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-10 20:35:06,320: INFO: 2211133503]: Accelerate version: 1.10.1\n"
     ]
    },
    {
     "ename": "InvalidVersion",
     "evalue": "Invalid version: 'N/A'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidVersion\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_accelerate_available\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# If still not detected, we'll use a workaround\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_accelerate_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33mAccelerate not detected by transformers, applying workaround...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# Import required classes with a different approach\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:399\u001b[39m, in \u001b[36mis_accelerate_available\u001b[39m\u001b[34m(min_version)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_accelerate_available\u001b[39m(min_version: \u001b[38;5;28mstr\u001b[39m = ACCELERATE_MIN_VERSION) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _accelerate_available \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mversion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accelerate_version\u001b[49m\u001b[43m)\u001b[49m >= version.parse(min_version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\packaging\\version.py:56\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(version)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(version: \u001b[38;5;28mstr\u001b[39m) -> Version:\n\u001b[32m     48\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the given version string.\u001b[39;00m\n\u001b[32m     49\u001b[39m \n\u001b[32m     50\u001b[39m \u001b[33;03m    >>> parse('1.0.dev1')\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m \u001b[33;03m    :raises InvalidVersion: When the version string is not a valid version.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\packaging\\version.py:202\u001b[39m, in \u001b[36mVersion.__init__\u001b[39m\u001b[34m(self, version)\u001b[39m\n\u001b[32m    200\u001b[39m match = \u001b[38;5;28mself\u001b[39m._regex.search(version)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidVersion(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# Store the parsed out pieces of the version\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;28mself\u001b[39m._version = _Version(\n\u001b[32m    206\u001b[39m     epoch=\u001b[38;5;28mint\u001b[39m(match.group(\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m match.group(\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m    207\u001b[39m     release=\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m match.group(\u001b[33m\"\u001b[39m\u001b[33mrelease\u001b[39m\u001b[33m\"\u001b[39m).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m    213\u001b[39m     local=_parse_local_version(match.group(\u001b[33m\"\u001b[39m\u001b[33mlocal\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m    214\u001b[39m )\n",
      "\u001b[31mInvalidVersion\u001b[39m: Invalid version: 'N/A'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# Set environment variables before importing transformers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int  \n",
    "    weight_decay: float\n",
    "    learning_rate: float  \n",
    "    logging_steps: int\n",
    "    eval_strategy: str\n",
    "    eval_steps: int\n",
    "    save_strategy: str \n",
    "    save_steps: int\n",
    "    save_total_limit: int  \n",
    "    gradient_accumulation_steps: int\n",
    "    lr_scheduler_type: str  \n",
    "    fp16: bool  \n",
    "    load_best_model_at_end: bool  \n",
    "    metric_for_best_model: str  \n",
    "    greater_is_better: bool  \n",
    "    prediction_loss_only: bool  \n",
    "    remove_unused_columns: bool  \n",
    "    report_to: str  \n",
    "\n",
    "from src.text_summarizer.logger import logger\n",
    "from src.text_summarizer.utils.common import read_yaml, create_directories\n",
    "from src.text_summarizer.constants import *\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        # Check if config files exist\n",
    "        if not os.path.exists(config_filepath):\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {config_filepath}\")\n",
    "        if not os.path.exists(params_filepath):\n",
    "            raise FileNotFoundError(f\"Parameters file not found: {params_filepath}\")\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Check if artifacts_root exists in config\n",
    "        if not hasattr(self.config, 'artifacts_root'):\n",
    "            # Add artifacts_root if it doesn't exist\n",
    "            self.config.artifacts_root = \"artifacts\"\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "        \n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        # Convert string values to appropriate types\n",
    "        model_trainer_config = ModelTrainingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(config.data_path),\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            num_train_epochs=int(params.num_train_epochs),\n",
    "            warmup_steps=int(params.warmup_steps),\n",
    "            per_device_train_batch_size=int(params.per_device_train_batch_size),\n",
    "            per_device_eval_batch_size=int(params.per_device_eval_batch_size),  \n",
    "            weight_decay=float(params.weight_decay),\n",
    "            learning_rate=float(params.learning_rate),  \n",
    "            logging_steps=int(params.logging_steps),\n",
    "            eval_strategy=params.eval_strategy,\n",
    "            eval_steps=int(params.eval_steps),\n",
    "            save_strategy=params.save_strategy,  \n",
    "            save_steps=int(params.save_steps),\n",
    "            save_total_limit=int(params.save_total_limit),  \n",
    "            gradient_accumulation_steps=int(params.gradient_accumulation_steps),\n",
    "            lr_scheduler_type=params.lr_scheduler_type,  \n",
    "            fp16=bool(params.fp16),  \n",
    "            load_best_model_at_end=bool(params.load_best_model_at_end),  \n",
    "            metric_for_best_model=params.metric_for_best_model,  \n",
    "            greater_is_better=bool(params.greater_is_better),  \n",
    "            prediction_loss_only=bool(params.prediction_loss_only),  \n",
    "            remove_unused_columns=bool(params.remove_unused_columns),  \n",
    "            report_to=params.report_to  \n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Test accelerate import and force proper detection\n",
    "try:\n",
    "    import accelerate\n",
    "    logger.info(f\"Accelerate version: {accelerate.__version__}\")\n",
    "    \n",
    "    # Force the transformers library to recognize accelerate properly\n",
    "    import transformers\n",
    "    from transformers.utils import is_accelerate_available\n",
    "    \n",
    "    # If still not detected, we'll use a workaround\n",
    "    if not is_accelerate_available():\n",
    "        logger.warning(\"Accelerate not detected by transformers, applying workaround...\")\n",
    "        \n",
    "        # Import required classes with a different approach\n",
    "        from transformers import (\n",
    "            AutoTokenizer, \n",
    "            AutoModelForSeq2SeqLM,\n",
    "            DataCollatorForSeq2Seq\n",
    "        )\n",
    "        \n",
    "        # Use a custom trainer or manual training loop\n",
    "        use_manual_training = True\n",
    "    else:\n",
    "        logger.info(\"Accelerate properly detected by transformers\")\n",
    "        from transformers import (\n",
    "            AutoTokenizer, \n",
    "            AutoModelForSeq2SeqLM,\n",
    "            Trainer, \n",
    "            TrainingArguments, \n",
    "            DataCollatorForSeq2Seq\n",
    "        )\n",
    "        use_manual_training = False\n",
    "        \n",
    "except ImportError as e:\n",
    "    logger.error(f\"Failed to import required packages: {e}\")\n",
    "    raise\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def train(self):\n",
    "        try:\n",
    "            # Set up device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            logger.info(f\"Using device: {device}\")\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            logger.info(f\"Loading model from: {self.config.model_ckpt}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "            model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "            \n",
    "            # Data collator\n",
    "            seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "            \n",
    "            # Load dataset\n",
    "            logger.info(f\"Loading dataset from: {self.config.data_path}\")\n",
    "            dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "            \n",
    "            # Check if we need to use manual training or can use Trainer\n",
    "            try:\n",
    "                # Try to use the standard Trainer\n",
    "                self._train_with_trainer(model_pegasus, tokenizer, seq2seq_data_collator, dataset_samsum_pt, device)\n",
    "            except Exception as trainer_error:\n",
    "                logger.warning(f\"Standard Trainer failed: {trainer_error}\")\n",
    "                logger.info(\"Falling back to manual training loop...\")\n",
    "                self._train_manually(model_pegasus, tokenizer, seq2seq_data_collator, dataset_samsum_pt, device)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _train_with_trainer(self, model, tokenizer, data_collator, dataset, device):\n",
    "        \"\"\"Standard training with HuggingFace Trainer\"\"\"\n",
    "        from transformers import Trainer, TrainingArguments\n",
    "        \n",
    "        # Create logs directory\n",
    "        logs_dir = os.path.join(self.config.root_dir, \"logs\")\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        \n",
    "        # Training arguments\n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            eval_strategy=self.config.eval_strategy,\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            save_strategy=self.config.save_strategy,\n",
    "            save_steps=self.config.save_steps,\n",
    "            prediction_loss_only=True,\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_num_workers=0,\n",
    "            dataloader_pin_memory=False,\n",
    "            logging_dir=logs_dir,\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        logger.info(\"Initializing Trainer...\")\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=trainer_args,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"]\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        logger.info(\"Starting training with Trainer...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the model\n",
    "        logger.info(\"Saving model...\")\n",
    "        trainer.save_model()\n",
    "        \n",
    "        self._save_model_and_tokenizer(model, tokenizer)\n",
    "        logger.info(\"Training with Trainer completed successfully!\")\n",
    "\n",
    "    def _train_manually(self, model, tokenizer, data_collator, dataset, device):\n",
    "        \"\"\"Manual training loop as fallback\"\"\"\n",
    "        from torch.utils.data import DataLoader\n",
    "        from transformers import get_linear_schedule_with_warmup\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset[\"train\"], \n",
    "            batch_size=self.config.per_device_train_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=data_collator,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            dataset[\"validation\"], \n",
    "            batch_size=self.config.per_device_eval_batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=data_collator,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        total_steps = len(train_dataloader) * self.config.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        global_step = 0\n",
    "        \n",
    "        logger.info(\"Starting manual training...\")\n",
    "        for epoch in range(self.config.num_train_epochs):\n",
    "            logger.info(f\"Epoch {epoch + 1}/{self.config.num_train_epochs}\")\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update weights\n",
    "                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    global_step += 1\n",
    "                    \n",
    "                    # Logging\n",
    "                    if global_step % self.config.logging_steps == 0:\n",
    "                        logger.info(f\"Step {global_step}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'avg_loss': f'{epoch_loss/(batch_idx+1):.4f}'\n",
    "                })\n",
    "                \n",
    "                # Evaluation\n",
    "                if global_step % self.config.eval_steps == 0:\n",
    "                    eval_loss = self._evaluate_manually(model, val_dataloader, device)\n",
    "                    logger.info(f\"Evaluation Loss at step {global_step}: {eval_loss:.4f}\")\n",
    "                    model.train()  # Back to training mode\n",
    "        \n",
    "        # Save model\n",
    "        self._save_model_and_tokenizer(model, tokenizer)\n",
    "        logger.info(\"Manual training completed successfully!\")\n",
    "\n",
    "    def _evaluate_manually(self, model, dataloader, device):\n",
    "        \"\"\"Manual evaluation\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                total_loss += outputs.loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "    def _save_model_and_tokenizer(self, model, tokenizer):\n",
    "        \"\"\"Save model and tokenizer\"\"\"\n",
    "        model_dir = os.path.join(self.config.root_dir, \"pegasus_model\")\n",
    "        tokenizer_dir = os.path.join(self.config.root_dir, \"tokenizer\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "        \n",
    "        model.save_pretrained(model_dir)\n",
    "        tokenizer.save_pretrained(tokenizer_dir)\n",
    "        \n",
    "        logger.info(f\"Model saved at: {model_dir}\")\n",
    "        logger.info(f\"Tokenizer saved at: {tokenizer_dir}\")\n",
    "\n",
    "# Main execution\n",
    "\n",
    "try: \n",
    "    config_manager = ConfigurationManager()\n",
    "    model_trainer_config = config_manager.get_model_training_config()\n",
    "    model_trainer = ModelTrainer(model_trainer_config)\n",
    "    model_trainer.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error training model: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0dbba0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 1.10.1\n",
      "[2025-09-10 20:37:14,240: INFO: 3835435508]: Accelerate available after patch: True\n",
      "[2025-09-10 20:37:14,241: INFO: 3835435508]: Starting model training pipeline...\n",
      "[2025-09-10 20:37:14,244: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2025-09-10 20:37:14,244: INFO: common]: yaml file: config\\params.yaml loaded successfully\n",
      "[2025-09-10 20:37:14,244: INFO: common]: Directory created at: artifacts\n",
      "[2025-09-10 20:37:14,244: INFO: common]: Directory created at: artifacts/model_trainer\n",
      "[2025-09-10 20:37:14,244: INFO: 3835435508]: Using device: cpu\n",
      "[2025-09-10 20:37:14,250: INFO: 3835435508]: Loading model from: google/pegasus-cnn_dailymail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-10 20:37:20,440: INFO: 3835435508]: Loading dataset from: artifacts\\data_transformation\\samsum_dataset\n",
      "[2025-09-10 20:37:20,456: ERROR: 3835435508]: Error during training: name 'AcceleratorConfig' is not defined\n",
      "[2025-09-10 20:37:20,456: ERROR: 3835435508]: Error training model: name 'AcceleratorConfig' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AcceleratorConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 256\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    255\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError training model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 251\u001b[39m\n\u001b[32m    249\u001b[39m     model_trainer_config = config_manager.get_model_training_config()\n\u001b[32m    250\u001b[39m     model_trainer = ModelTrainer(model_trainer_config)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     \u001b[43mmodel_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mModel training pipeline completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 171\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    168\u001b[39m os.makedirs(logs_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Training arguments - simplified to ensure they work\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m trainer_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Avoid multiprocessing issues\u001b[39;49;00m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_pin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Avoid memory issues\u001b[39;49;00m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogs_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensorboard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Disable some features that might cause issues\u001b[39;49;00m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhub_model_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable FP16 to avoid potential issues\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable BF16\u001b[39;49;00m\n\u001b[32m    197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Initialize trainer\u001b[39;00m\n\u001b[32m    200\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mInitializing Trainer...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:134\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\training_args.py:1781\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# We need to setup the accelerator config here *before* the first call to `self.device`\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m1781\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.accelerator_config, \u001b[43mAcceleratorConfig\u001b[49m):\n\u001b[32m   1782\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1783\u001b[39m             \u001b[38;5;28mself\u001b[39m.accelerator_config = AcceleratorConfig()\n",
      "\u001b[31mNameError\u001b[39m: name 'AcceleratorConfig' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# Set environment variables before importing anything\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "\n",
    "# Import accelerate first and patch the version issue\n",
    "import accelerate\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "\n",
    "# Now patch the transformers import_utils BEFORE importing transformers\n",
    "import transformers.utils.import_utils as import_utils\n",
    "\n",
    "# Store the original version\n",
    "_original_accelerate_version = accelerate.__version__\n",
    "\n",
    "# Monkey patch the version detection to return the actual version\n",
    "import_utils._accelerate_available = True\n",
    "import_utils._accelerate_version = _original_accelerate_version\n",
    "\n",
    "# Also patch the version parsing function to handle our case\n",
    "original_is_accelerate_available = import_utils.is_accelerate_available\n",
    "\n",
    "def patched_is_accelerate_available(min_version=\"0.26.0\"):\n",
    "    \"\"\"Always return True since we know accelerate is installed\"\"\"\n",
    "    try:\n",
    "        # Import packaging.version here to avoid circular imports\n",
    "        from packaging import version\n",
    "        return version.parse(_original_accelerate_version) >= version.parse(min_version)\n",
    "    except:\n",
    "        # If version parsing fails, just return True since we know it's installed\n",
    "        return True\n",
    "\n",
    "# Apply the patch\n",
    "import_utils.is_accelerate_available = patched_is_accelerate_available\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int  \n",
    "    weight_decay: float\n",
    "    learning_rate: float  \n",
    "    logging_steps: int\n",
    "    eval_strategy: str\n",
    "    eval_steps: int\n",
    "    save_strategy: str \n",
    "    save_steps: int\n",
    "    save_total_limit: int  \n",
    "    gradient_accumulation_steps: int\n",
    "    lr_scheduler_type: str  \n",
    "    fp16: bool  \n",
    "    load_best_model_at_end: bool  \n",
    "    metric_for_best_model: str  \n",
    "    greater_is_better: bool  \n",
    "    prediction_loss_only: bool  \n",
    "    remove_unused_columns: bool  \n",
    "    report_to: str  \n",
    "\n",
    "from src.text_summarizer.logger import logger\n",
    "from src.text_summarizer.utils.common import read_yaml, create_directories\n",
    "from src.text_summarizer.constants import *\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        # Check if config files exist\n",
    "        if not os.path.exists(config_filepath):\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {config_filepath}\")\n",
    "        if not os.path.exists(params_filepath):\n",
    "            raise FileNotFoundError(f\"Parameters file not found: {params_filepath}\")\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Check if artifacts_root exists in config\n",
    "        if not hasattr(self.config, 'artifacts_root'):\n",
    "            # Add artifacts_root if it doesn't exist\n",
    "            self.config.artifacts_root = \"artifacts\"\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "        \n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        # Convert string values to appropriate types\n",
    "        model_trainer_config = ModelTrainingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(config.data_path),\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            num_train_epochs=int(params.num_train_epochs),\n",
    "            warmup_steps=int(params.warmup_steps),\n",
    "            per_device_train_batch_size=int(params.per_device_train_batch_size),\n",
    "            per_device_eval_batch_size=int(params.per_device_eval_batch_size),  \n",
    "            weight_decay=float(params.weight_decay),\n",
    "            learning_rate=float(params.learning_rate),  \n",
    "            logging_steps=int(params.logging_steps),\n",
    "            eval_strategy=params.eval_strategy,\n",
    "            eval_steps=int(params.eval_steps),\n",
    "            save_strategy=params.save_strategy,  \n",
    "            save_steps=int(params.save_steps),\n",
    "            save_total_limit=int(params.save_total_limit),  \n",
    "            gradient_accumulation_steps=int(params.gradient_accumulation_steps),\n",
    "            lr_scheduler_type=params.lr_scheduler_type,  \n",
    "            fp16=bool(params.fp16),  \n",
    "            load_best_model_at_end=bool(params.load_best_model_at_end),  \n",
    "            metric_for_best_model=params.metric_for_best_model,  \n",
    "            greater_is_better=bool(params.greater_is_better),  \n",
    "            prediction_loss_only=bool(params.prediction_loss_only),  \n",
    "            remove_unused_columns=bool(params.remove_unused_columns),  \n",
    "            report_to=params.report_to  \n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "\n",
    "# Now we can safely import transformers\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# Verify the patch worked\n",
    "from transformers.utils import is_accelerate_available\n",
    "logger.info(f\"Accelerate available after patch: {is_accelerate_available()}\")\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def train(self):\n",
    "        try:\n",
    "            # Set up device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            logger.info(f\"Using device: {device}\")\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            logger.info(f\"Loading model from: {self.config.model_ckpt}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "            model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "            \n",
    "            # Data collator\n",
    "            seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "            \n",
    "            # Load dataset\n",
    "            logger.info(f\"Loading dataset from: {self.config.data_path}\")\n",
    "            dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "            \n",
    "            # Create logs directory\n",
    "            logs_dir = os.path.join(self.config.root_dir, \"logs\")\n",
    "            os.makedirs(logs_dir, exist_ok=True)\n",
    "            \n",
    "            # Training arguments - simplified to ensure they work\n",
    "            trainer_args = TrainingArguments(\n",
    "                output_dir=self.config.root_dir,\n",
    "                num_train_epochs=self.config.num_train_epochs,\n",
    "                per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "                per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "                warmup_steps=self.config.warmup_steps,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "                learning_rate=self.config.learning_rate,\n",
    "                logging_steps=self.config.logging_steps,\n",
    "                eval_strategy=self.config.eval_strategy,\n",
    "                eval_steps=self.config.eval_steps,\n",
    "                save_strategy=self.config.save_strategy,\n",
    "                save_steps=self.config.save_steps,\n",
    "                save_total_limit=self.config.save_total_limit,\n",
    "                prediction_loss_only=self.config.prediction_loss_only,\n",
    "                remove_unused_columns=self.config.remove_unused_columns,\n",
    "                dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "                dataloader_pin_memory=False,  # Avoid memory issues\n",
    "                logging_dir=logs_dir,\n",
    "                report_to=self.config.report_to if self.config.report_to != \"tensorboard\" else \"none\",\n",
    "                # Disable some features that might cause issues\n",
    "                push_to_hub=False,\n",
    "                hub_model_id=None,\n",
    "                gradient_checkpointing=False,\n",
    "                fp16=False,  # Disable FP16 to avoid potential issues\n",
    "                bf16=False,  # Disable BF16\n",
    "            )\n",
    "            \n",
    "            # Initialize trainer\n",
    "            logger.info(\"Initializing Trainer...\")\n",
    "            trainer = Trainer(\n",
    "                model=model_pegasus,\n",
    "                args=trainer_args,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=seq2seq_data_collator,\n",
    "                train_dataset=dataset_samsum_pt[\"train\"],\n",
    "                eval_dataset=dataset_samsum_pt[\"validation\"]\n",
    "            )\n",
    "            \n",
    "            # Start training\n",
    "            logger.info(\"Starting training...\")\n",
    "            trainer.train()\n",
    "            \n",
    "            # Save the model\n",
    "            logger.info(\"Saving model...\")\n",
    "            trainer.save_model()\n",
    "            \n",
    "            # Save model and tokenizer separately\n",
    "            model_dir = os.path.join(self.config.root_dir, \"pegasus_model\")\n",
    "            tokenizer_dir = os.path.join(self.config.root_dir, \"tokenizer\")\n",
    "            \n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "            \n",
    "            model_pegasus.save_pretrained(model_dir)\n",
    "            tokenizer.save_pretrained(tokenizer_dir)\n",
    "            \n",
    "            logger.info(\"Model training completed successfully!\")\n",
    "            logger.info(f\"Model saved at: {model_dir}\")\n",
    "            logger.info(f\"Tokenizer saved at: {tokenizer_dir}\")\n",
    "            \n",
    "            # Log final metrics if available\n",
    "            if hasattr(trainer, 'state') and trainer.state.log_history:\n",
    "                try:\n",
    "                    final_metrics = trainer.state.log_history[-1]\n",
    "                    logger.info(f\"Final training metrics: {final_metrics}\")\n",
    "                except:\n",
    "                    logger.info(\"Training completed but final metrics not available\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Main execution\n",
    "\n",
    "try: \n",
    "    logger.info(\"Starting model training pipeline...\")\n",
    "    config_manager = ConfigurationManager()\n",
    "    model_trainer_config = config_manager.get_model_training_config()\n",
    "    model_trainer = ModelTrainer(model_trainer_config)\n",
    "    model_trainer.train()\n",
    "    logger.info(\"Model training pipeline completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error training model: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "437c2f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-10 23:28:56,903: INFO: 654578579]: Starting manual model training pipeline...\n",
      "[2025-09-10 23:28:56,903: INFO: 654578579]: This approach bypasses HuggingFace Trainer to avoid dependency issues\n",
      "[2025-09-10 23:28:56,918: INFO: common]: yaml file: config\\config.yaml loaded successfully\n",
      "[2025-09-10 23:28:56,934: INFO: common]: yaml file: config\\params.yaml loaded successfully\n",
      "[2025-09-10 23:28:56,936: INFO: common]: Directory created at: artifacts\n",
      "[2025-09-10 23:28:56,937: INFO: common]: Directory created at: artifacts/model_trainer\n",
      "[2025-09-10 23:28:56,939: INFO: 654578579]: Using device: cpu\n",
      "[2025-09-10 23:28:56,940: INFO: 654578579]: Loading model from: google/pegasus-cnn_dailymail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-10 23:29:08,346: INFO: 654578579]: Loading dataset from: artifacts\\data_transformation\\samsum_dataset\n",
      "[2025-09-10 23:29:08,431: INFO: 654578579]: Dataset structure:\n",
      "[2025-09-10 23:29:08,432: INFO: 654578579]:   train: 14732 examples\n",
      "[2025-09-10 23:29:08,432: INFO: 654578579]:   test: 819 examples\n",
      "[2025-09-10 23:29:08,433: INFO: 654578579]:   validation: 818 examples\n",
      "[2025-09-10 23:29:08,435: INFO: 654578579]: Sample keys: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels']\n",
      "[2025-09-10 23:29:08,435: INFO: 654578579]:   id: str\n",
      "[2025-09-10 23:29:08,436: INFO: 654578579]:   dialogue: str\n",
      "[2025-09-10 23:29:08,436: INFO: 654578579]:   summary: str\n",
      "[2025-09-10 23:29:08,437: INFO: 654578579]:   input_ids: length=28 type=list\n",
      "[2025-09-10 23:29:08,438: INFO: 654578579]:   attention_mask: length=28 type=list\n",
      "[2025-09-10 23:29:08,439: INFO: 654578579]:   labels: length=11 type=list\n",
      "[2025-09-10 23:29:08,440: INFO: 654578579]: Removed from train: ['id', 'dialogue', 'summary']\n",
      "[2025-09-10 23:29:08,442: INFO: 654578579]: Removed from test: ['id', 'dialogue', 'summary']\n",
      "[2025-09-10 23:29:08,444: INFO: 654578579]: Removed from validation: ['id', 'dialogue', 'summary']\n",
      "[2025-09-10 23:29:08,445: INFO: 654578579]: Remaining columns: ['input_ids', 'attention_mask', 'labels']\n",
      "[2025-09-10 23:29:08,447: INFO: 654578579]: Starting training for 3 epochs...\n",
      "[2025-09-10 23:29:08,448: INFO: 654578579]: Total training steps: 11049\n",
      "[2025-09-10 23:29:08,450: INFO: 654578579]: === Epoch 1/3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|         | 142/3683 [22:29<9:21:02,  9.51s/it, loss=11.7244, avg_loss=11.7455, lr=3.50e-06] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 370\u001b[39m\n\u001b[32m    368\u001b[39m     model_trainer_config = config_manager.get_model_training_config()\n\u001b[32m    369\u001b[39m     model_trainer = ManualModelTrainer(model_trainer_config)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m     \u001b[43mmodel_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mManual model training pipeline completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 213\u001b[39m, in \u001b[36mManualModelTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m batch = {k: v.to(\u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items()}\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m loss = outputs.loss / \u001b[38;5;28mself\u001b[39m.config.gradient_accumulation_steps\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1472\u001b[39m, in \u001b[36mPegasusForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1468\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1469\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1470\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1472\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1473\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1479\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1480\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1481\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1482\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1484\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1485\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1490\u001b[39m lm_logits = \u001b[38;5;28mself\u001b[39m.lm_head(outputs[\u001b[32m0\u001b[39m]) + \u001b[38;5;28mself\u001b[39m.final_logits_bias\n\u001b[32m   1492\u001b[39m masked_lm_loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1280\u001b[39m, in \u001b[36mPegasusModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1277\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1280\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[32m   1290\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:844\u001b[39m, in \u001b[36mPegasusEncoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    842\u001b[39m     layer_outputs = (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m     layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    851\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:326\u001b[39m, in \u001b[36mPegasusEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[39m\n\u001b[32m    324\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.final_layer_norm(hidden_states)\n\u001b[32m    325\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.activation_fn(\u001b[38;5;28mself\u001b[39m.fc1(hidden_states))\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m hidden_states = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.fc2(hidden_states)\n\u001b[32m    328\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\p1a2r\\OneDrive\\Desktop\\Git Hub Projects\\Text-Summarizer\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1422\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int  \n",
    "    weight_decay: float\n",
    "    learning_rate: float  \n",
    "    logging_steps: int\n",
    "    eval_strategy: str\n",
    "    eval_steps: int\n",
    "    save_strategy: str \n",
    "    save_steps: int\n",
    "    save_total_limit: int  \n",
    "    gradient_accumulation_steps: int\n",
    "    lr_scheduler_type: str  \n",
    "    fp16: bool  \n",
    "    load_best_model_at_end: bool  \n",
    "    metric_for_best_model: str  \n",
    "    greater_is_better: bool  \n",
    "    prediction_loss_only: bool  \n",
    "    remove_unused_columns: bool  \n",
    "    report_to: str  \n",
    "\n",
    "from src.text_summarizer.logger import logger\n",
    "from src.text_summarizer.utils.common import read_yaml, create_directories\n",
    "from src.text_summarizer.constants import *\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        if not os.path.exists(config_filepath):\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {config_filepath}\")\n",
    "        if not os.path.exists(params_filepath):\n",
    "            raise FileNotFoundError(f\"Parameters file not found: {params_filepath}\")\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        if not hasattr(self.config, 'artifacts_root'):\n",
    "            self.config.artifacts_root = \"artifacts\"\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(config.data_path),\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            num_train_epochs=int(params.num_train_epochs),\n",
    "            warmup_steps=int(params.warmup_steps),\n",
    "            per_device_train_batch_size=int(params.per_device_train_batch_size),\n",
    "            per_device_eval_batch_size=int(params.per_device_eval_batch_size),  \n",
    "            weight_decay=float(params.weight_decay),\n",
    "            learning_rate=float(params.learning_rate),  \n",
    "            logging_steps=int(params.logging_steps),\n",
    "            eval_strategy=params.eval_strategy,\n",
    "            eval_steps=int(params.eval_steps),\n",
    "            save_strategy=params.save_strategy,  \n",
    "            save_steps=int(params.save_steps),\n",
    "            save_total_limit=int(params.save_total_limit),  \n",
    "            gradient_accumulation_steps=int(params.gradient_accumulation_steps),\n",
    "            lr_scheduler_type=params.lr_scheduler_type,  \n",
    "            fp16=bool(params.fp16),  \n",
    "            load_best_model_at_end=bool(params.load_best_model_at_end),  \n",
    "            metric_for_best_model=params.metric_for_best_model,  \n",
    "            greater_is_better=bool(params.greater_is_better),  \n",
    "            prediction_loss_only=bool(params.prediction_loss_only),  \n",
    "            remove_unused_columns=bool(params.remove_unused_columns),  \n",
    "            report_to=params.report_to  \n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "\n",
    "# Import only what we need - no Trainer class\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "class ManualModelTrainer:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def train(self):\n",
    "        try:\n",
    "            logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "            # Load model and tokenizer\n",
    "            logger.info(f\"Loading model from: {self.config.model_ckpt}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(self.device)\n",
    "\n",
    "            # Load dataset\n",
    "            logger.info(f\"Loading dataset from: {self.config.data_path}\")\n",
    "            dataset = load_from_disk(self.config.data_path)\n",
    "\n",
    "            # Debug: Check dataset structure\n",
    "            logger.info(\"Dataset structure:\")\n",
    "            for split in dataset.keys():\n",
    "                logger.info(f\"  {split}: {len(dataset[split])} examples\")\n",
    "\n",
    "            # Check a sample from training data\n",
    "            sample = dataset[\"train\"][0]\n",
    "            logger.info(\"Sample keys: %s\", list(sample.keys()))\n",
    "            for key, value in sample.items():\n",
    "                if isinstance(value, (list, torch.Tensor)):\n",
    "                    logger.info(\"  %s: length=%d type=%s\", key, len(value), type(value).__name__)\n",
    "                else:\n",
    "                    logger.info(\"  %s: %s\", key, type(value).__name__)\n",
    "\n",
    "            # Verify tokenizer has pad token\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                logger.info(\"Set pad_token to eos_token\")\n",
    "\n",
    "            # Remove non-tensor columns completely\n",
    "            columns_to_remove = ['id', 'dialogue', 'summary']\n",
    "            for split in dataset.keys():\n",
    "                # Check which columns actually exist\n",
    "                existing_columns = set(dataset[split].column_names)\n",
    "                columns_to_remove_in_split = [col for col in columns_to_remove if col in existing_columns]\n",
    "                \n",
    "                if columns_to_remove_in_split:\n",
    "                    dataset[split] = dataset[split].remove_columns(columns_to_remove_in_split)\n",
    "                    logger.info(\"Removed from %s: %s\", split, columns_to_remove_in_split)\n",
    "\n",
    "            logger.info(\"Remaining columns: %s\", list(dataset[\"train\"].column_names))\n",
    "\n",
    "            # Data collator\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "            # Create data loaders\n",
    "            train_dataloader = DataLoader(\n",
    "                dataset[\"train\"], \n",
    "                batch_size=self.config.per_device_train_batch_size,\n",
    "                shuffle=True,\n",
    "                collate_fn=data_collator,\n",
    "                num_workers=0,\n",
    "                pin_memory=False\n",
    "            )\n",
    "\n",
    "            val_dataloader = DataLoader(\n",
    "                dataset[\"validation\"], \n",
    "                batch_size=self.config.per_device_eval_batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=data_collator,\n",
    "                num_workers=0,\n",
    "                pin_memory=False\n",
    "            )\n",
    "\n",
    "            \n",
    "            # Optimizer and scheduler\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                model.parameters(), \n",
    "                lr=self.config.learning_rate,\n",
    "                weight_decay=self.config.weight_decay\n",
    "            )\n",
    "            \n",
    "            total_steps = len(train_dataloader) * self.config.num_train_epochs\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.config.warmup_steps,\n",
    "                num_training_steps=total_steps\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Starting training for {self.config.num_train_epochs} epochs...\")\n",
    "            logger.info(f\"Total training steps: {total_steps}\")\n",
    "            \n",
    "            # Training metrics tracking\n",
    "            training_history = []\n",
    "            best_eval_loss = float('inf')\n",
    "            global_step = 0\n",
    "            \n",
    "            # Training loop\n",
    "            model.train()\n",
    "            for epoch in range(self.config.num_train_epochs):\n",
    "                logger.info(f\"=== Epoch {epoch + 1}/{self.config.num_train_epochs} ===\")\n",
    "                \n",
    "                epoch_loss = 0\n",
    "                num_batches = 0\n",
    "                \n",
    "                progress_bar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "                \n",
    "                for batch_idx, batch in enumerate(progress_bar):\n",
    "                    # Move batch to device\n",
    "                    batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss / self.config.gradient_accumulation_steps\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    epoch_loss += loss.item() * self.config.gradient_accumulation_steps\n",
    "                    num_batches += 1\n",
    "                    \n",
    "                    # Update weights\n",
    "                    if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        \n",
    "                        global_step += 1\n",
    "                        \n",
    "                        # Logging\n",
    "                        if global_step % self.config.logging_steps == 0:\n",
    "                            current_lr = scheduler.get_last_lr()[0]\n",
    "                            logger.info(f\"Step {global_step}/{total_steps} | Loss: {loss.item() * self.config.gradient_accumulation_steps:.4f} | LR: {current_lr:.2e}\")\n",
    "                        \n",
    "                        # Evaluation\n",
    "                        if global_step % self.config.eval_steps == 0:\n",
    "                            eval_loss = self.evaluate(model, val_dataloader)\n",
    "                            logger.info(f\"Evaluation at step {global_step} | Eval Loss: {eval_loss:.4f}\")\n",
    "                            \n",
    "                            # Save best model\n",
    "                            if eval_loss < best_eval_loss:\n",
    "                                best_eval_loss = eval_loss\n",
    "                                logger.info(f\"New best model! Eval loss: {eval_loss:.4f}\")\n",
    "                                self.save_checkpoint(model, tokenizer, global_step, eval_loss, is_best=True)\n",
    "                            \n",
    "                            # Record metrics\n",
    "                            training_history.append({\n",
    "                                'step': global_step,\n",
    "                                'epoch': epoch + 1,\n",
    "                                'train_loss': loss.item() * self.config.gradient_accumulation_steps,\n",
    "                                'eval_loss': eval_loss,\n",
    "                                'learning_rate': current_lr\n",
    "                            })\n",
    "                            \n",
    "                            model.train()  # Back to training mode\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': f'{loss.item() * self.config.gradient_accumulation_steps:.4f}',\n",
    "                        'avg_loss': f'{epoch_loss/num_batches:.4f}',\n",
    "                        'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "                    })\n",
    "                \n",
    "                avg_epoch_loss = epoch_loss / num_batches\n",
    "                logger.info(f\"Epoch {epoch + 1} completed | Average Loss: {avg_epoch_loss:.4f}\")\n",
    "                \n",
    "                # End of epoch evaluation\n",
    "                eval_loss = self.evaluate(model, val_dataloader)\n",
    "                logger.info(f\"End of Epoch {epoch + 1} | Eval Loss: {eval_loss:.4f}\")\n",
    "                \n",
    "                if eval_loss < best_eval_loss:\n",
    "                    best_eval_loss = eval_loss\n",
    "                    logger.info(f\"New best model at end of epoch {epoch + 1}! Eval loss: {eval_loss:.4f}\")\n",
    "                    self.save_checkpoint(model, tokenizer, global_step, eval_loss, is_best=True)\n",
    "            \n",
    "            # Final save\n",
    "            logger.info(\"Training completed! Saving final model...\")\n",
    "            self.save_final_model(model, tokenizer, training_history, best_eval_loss)\n",
    "            \n",
    "            logger.info(\"=\"*50)\n",
    "            logger.info(\"TRAINING SUMMARY\")\n",
    "            logger.info(f\"Total epochs: {self.config.num_train_epochs}\")\n",
    "            logger.info(f\"Total steps: {global_step}\")\n",
    "            logger.info(f\"Best evaluation loss: {best_eval_loss:.4f}\")\n",
    "            logger.info(f\"Final learning rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "            logger.info(\"=\"*50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def evaluate(self, model, dataloader):\n",
    "        \"\"\"Evaluate the model\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        logger.info(\"Running evaluation...\")\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                total_loss += outputs.loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        return avg_loss\n",
    "\n",
    "    def save_checkpoint(self, model, tokenizer, step, eval_loss, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint_dir = os.path.join(self.config.root_dir, f\"checkpoint-{step}\")\n",
    "        if is_best:\n",
    "            checkpoint_dir = os.path.join(self.config.root_dir, \"best_model\")\n",
    "        \n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        model.save_pretrained(checkpoint_dir)\n",
    "        tokenizer.save_pretrained(checkpoint_dir)\n",
    "        \n",
    "        # Save checkpoint info\n",
    "        checkpoint_info = {\n",
    "            'step': step,\n",
    "            'eval_loss': eval_loss,\n",
    "            'is_best': is_best\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(checkpoint_dir, 'checkpoint_info.json'), 'w') as f:\n",
    "            json.dump(checkpoint_info, f, indent=2)\n",
    "\n",
    "    def save_final_model(self, model, tokenizer, training_history, best_eval_loss):\n",
    "        \"\"\"Save final model and training artifacts\"\"\"\n",
    "        # Save final model\n",
    "        model_dir = os.path.join(self.config.root_dir, \"final_model\")\n",
    "        tokenizer_dir = os.path.join(self.config.root_dir, \"tokenizer\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "        \n",
    "        model.save_pretrained(model_dir)\n",
    "        tokenizer.save_pretrained(tokenizer_dir)\n",
    "        \n",
    "        # Save training history\n",
    "        with open(os.path.join(self.config.root_dir, \"training_history.json\"), \"w\") as f:\n",
    "            json.dump(training_history, f, indent=2)\n",
    "        \n",
    "        # Save training summary\n",
    "        summary = {\n",
    "            \"best_eval_loss\": best_eval_loss,\n",
    "            \"total_epochs\": self.config.num_train_epochs,\n",
    "            \"model_name\": self.config.model_ckpt,\n",
    "            \"training_completed\": True\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.config.root_dir, \"training_summary.json\"), \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Final model saved to: {model_dir}\")\n",
    "        logger.info(f\"Tokenizer saved to: {tokenizer_dir}\")\n",
    "        logger.info(f\"Training history saved to: {os.path.join(self.config.root_dir, 'training_history.json')}\")\n",
    "\n",
    "# Main execution\n",
    "\n",
    "try:\n",
    "    logger.info(\"Starting manual model training pipeline...\")\n",
    "    logger.info(\"This approach bypasses HuggingFace Trainer to avoid dependency issues\")\n",
    "    config_manager = ConfigurationManager()\n",
    "    model_trainer_config = config_manager.get_model_training_config()\n",
    "    model_trainer = ManualModelTrainer(model_trainer_config)\n",
    "    model_trainer.train()\n",
    "    logger.info(\"Manual model training pipeline completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error training model: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8c77a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
